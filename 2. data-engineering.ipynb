{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floating-modern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collective-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_static = pd.read_csv('raw_data/Static.csv', sep=';').drop('CLIENT_ID', axis=1)\n",
    "data_repay  = pd.read_csv('raw_data/Repayments.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "broad-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API for DataTable and feature engineering.\n",
    "\n",
    "class DataTable:\n",
    "    def __init__(self):\n",
    "        self.matrix = None\n",
    "        \n",
    "        self.log_payment_features = []\n",
    "        self.id_payment_features   = ['REPAYMENT_SCHEDULED']\n",
    "        self.id_relative_payment_features  = []\n",
    "        self.log_relative_payment_features = []\n",
    "        \n",
    "        self.id_target_exact_key  = 'REPAYMENT_ACTUAL'\n",
    "        self.log_target_exact_key = 'LOG_REPAYMENT_ACTUAL'\n",
    "        self.id_target_relative_key  = 'PERCENT_ACTUAL'\n",
    "        self.log_target_relative_key = 'LOG_PERCENT_ACTUAL'\n",
    "        \n",
    "        self.explicit_target_keys = [ \n",
    "            self.id_target_exact_key, self.log_target_exact_key, \n",
    "            self.id_target_relative_key, self.log_target_relative_key\n",
    "        ]\n",
    "\n",
    "        self.indicator_features   = ['GENDER']\n",
    "        self.id_numeric_features  = [\n",
    "            'PERIOD_ID', 'TERM', 'CONTRACT_SUM', 'AGE', \n",
    "            'LOAN_TO_INCOME', 'PAYMENT_TO_INCOME', 'DOWNPAYMENT', \n",
    "            'CAR_CATEGORY', 'GRACE_PERIOD', 'RATE_CHANGE_AFTER_GRACE'\n",
    "        ]\n",
    "        self.log_numeric_features = []\n",
    "        \n",
    "\n",
    "def _make_payment_cumulative_and_average_features(data_table):\n",
    "    data = data_table.matrix\n",
    "    group_structure = data.groupby('CONTRACT_ID')\n",
    "    \n",
    "    cumulative_sum_scheduled = list()\n",
    "    cumulative_sum_actual = list()\n",
    "    average_repayment_actual = list()\n",
    "    \n",
    "    for key, df in group_structure:\n",
    "        cumulative_sum_scheduled += df['REPAYMENT_SCHEDULED'].cumsum().tolist()\n",
    "        cumulative_sum_actual += (df['REPAYMENT_ACTUAL'].cumsum() - df['REPAYMENT_ACTUAL']).tolist()\n",
    "        \n",
    "        cumsum_actual = np.array(df['REPAYMENT_ACTUAL'].cumsum().tolist())\n",
    "        counts_actual = np.array(range(1, len(cumsum_actual) + 1))\n",
    "        average_actual = cumsum_actual / counts_actual\n",
    "        corrected_average_history = [0]\n",
    "        for item in average_actual.tolist():\n",
    "            corrected_average_history.append(item)\n",
    "        corrected_average_history.pop()\n",
    "        average_repayment_actual += corrected_average_history\n",
    "        \n",
    "    data['CUMSUM_REPAYMENT_SCHEDULED'] = cumulative_sum_scheduled\n",
    "    data['CUMSUM_REPAYMENT_ACTUAL'] = cumulative_sum_actual\n",
    "    data['AVERAGE_REPAYMENT_ACTUAL'] = average_repayment_actual\n",
    "    data_table.id_payment_features += [\n",
    "        'CUMSUM_REPAYMENT_SCHEDULED', 'CUMSUM_REPAYMENT_ACTUAL', 'AVERAGE_REPAYMENT_ACTUAL']\n",
    "        \n",
    "\n",
    "def _make_payment_history_features(data_table):\n",
    "    data = data_table.matrix\n",
    "    group_structure = data.groupby('CONTRACT_ID')\n",
    "    \n",
    "    lagged_actual_payments = defaultdict(list)\n",
    "    lagged_scheduled_payments = defaultdict(list)\n",
    "    \n",
    "    configurations = [\n",
    "        (lagged_actual_payments[1], 1, 'REPAYMENT_ACTUAL'),\n",
    "        (lagged_scheduled_payments[1], 1, 'REPAYMENT_SCHEDULED'),\n",
    "        (lagged_actual_payments[2], 2, 'REPAYMENT_ACTUAL'),\n",
    "        (lagged_scheduled_payments[2], 2, 'REPAYMENT_SCHEDULED')\n",
    "    ]\n",
    "        \n",
    "    for key, df in group_structure:\n",
    "        for list_holder, shift, key in configurations:\n",
    "            contract_list = df[key].shift(shift).tolist()\n",
    "            for idx in range(shift):\n",
    "                contract_list[idx] = df['REPAYMENT_SCHEDULED'].values[idx]\n",
    "            list_holder += contract_list\n",
    "    \n",
    "    for list_holder, shift, key in configurations:\n",
    "        new_key = 'HISTORY_SHIFT_{}_'.format(shift) + key\n",
    "        data[new_key] = list_holder\n",
    "        data_table.id_payment_features.append(new_key)\n",
    "        \n",
    "\n",
    "def _make_payment_relative_features(data_table):\n",
    "    matrix = data_table.matrix\n",
    "    exact_features = copy.deepcopy(data_table.id_payment_features)\n",
    "    for item in exact_features:\n",
    "        relative_key = 'RELATIVE_' + item\n",
    "        matrix['RELATIVE_' + item] = matrix[item] / matrix['CONTRACT_SUM'] * 100\n",
    "        data_table.id_relative_payment_features.append(relative_key)\n",
    "\n",
    "        \n",
    "def _make_payment_log_features(data_table):\n",
    "    matrix = data_table.matrix\n",
    "    configurations = [\n",
    "        (data_table.id_payment_features, data_table.log_payment_features),\n",
    "        (data_table.id_relative_payment_features, data_table.log_relative_payment_features)\n",
    "    ]\n",
    "    for source, target in configurations:\n",
    "        for feature_key in source:\n",
    "            log_key = 'LOG_' + feature_key\n",
    "            matrix[log_key] = np.log1p(matrix[feature_key])\n",
    "            target.append(log_key)\n",
    "    \n",
    "    \n",
    "def _make_regular_log_features(data_table):\n",
    "    matrix = data_table.matrix\n",
    "    for feature_key in data_table.id_numeric_features:\n",
    "        log_key = 'LOG_' + feature_key\n",
    "        matrix[log_key] = np.log1p(matrix[feature_key])\n",
    "        data_table.log_numeric_features.append(log_key)\n",
    "        \n",
    "        \n",
    "def _make_custom_features(data_table):\n",
    "    matrix = data_table.matrix\n",
    "    matrix['BEFORE_GRACE'] = matrix['PAYMENT_TO_INCOME'] / matrix['LOAN_TO_INCOME'] * 100\n",
    "    matrix['GRACE_ON'] = 1 * np.array(matrix['PERIOD_ID'] <= matrix['GRACE_PERIOD'])\n",
    "    \n",
    "    matrix['RATIO_5'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 5)\n",
    "    matrix['RATIO_10'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 10)\n",
    "    matrix['RATIO_20'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 20)\n",
    "    matrix['RATIO_30'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 30)\n",
    "    matrix['RATIO_40'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 40)\n",
    "    \n",
    "    matrix['IS_PERIOD_1'] = 1 * np.array(matrix['PERIOD_ID'] == 1)\n",
    "    matrix['IS_PERIOD_2'] = 1 * np.array(matrix['PERIOD_ID'] == 2)\n",
    "    matrix['IS_PERIOD_3'] = 1 * np.array(matrix['PERIOD_ID'] == 3)\n",
    "    \n",
    "    matrix['IS_CAR_1'] = 1 * np.array(matrix['CAR_CATEGORY'] == 1)\n",
    "    matrix['IS_CAR_2'] = 1 * np.array(matrix['CAR_CATEGORY'] == 2)\n",
    "    matrix['IS_CAR_3'] = 1 * np.array(matrix['CAR_CATEGORY'] == 3)\n",
    "    matrix['IS_CAR_4'] = 1 * np.array(matrix['CAR_CATEGORY'] == 4)\n",
    "    matrix['IS_CAR_5'] = 1 * np.array(matrix['CAR_CATEGORY'] == 5)\n",
    "    \n",
    "    data_table.id_numeric_features.append('BEFORE_GRACE')\n",
    "    data_table.indicator_features += ['GRACE_ON', 'RATIO_5', 'RATIO_10', 'RATIO_20', 'RATIO_30', 'RATIO_40'\n",
    "                                     'IS_PERIOD_1', 'IS_PERIOD_2', 'IS_PERIOD_3',\n",
    "                                     'IS_CAR_1', 'IS_CAR_2', 'IS_CAR_3', 'IS_CAR_4', 'IS_CAR_5']\n",
    "\n",
    "\n",
    "def _make_is_grace_constant_feature(data_table):\n",
    "    is_grace_constant = []\n",
    "    matrix = data_table.matrix\n",
    "    for key, df in matrix.groupby('CONTRACT_ID'):\n",
    "        df_grace_on = np.array(df['GRACE_ON'])\n",
    "        check_vals  = np.array(df['REPAYMENT_ACTUAL'])\n",
    "        grace_values = set()    \n",
    "        for on, val in zip(df_grace_on, check_vals):\n",
    "            if on == 1:\n",
    "                grace_values.add(val)\n",
    "        append = np.zeros_like(check_vals)\n",
    "        if len(grace_values) < 2:\n",
    "            append += 1\n",
    "        is_grace_constant += append.tolist()\n",
    "\n",
    "    matrix['IS_GRACE_CONSTANT'] = np.array(is_grace_constant)\n",
    "    data_table.indicator_features.append('IS_GRACE_CONSTANT')\n",
    "\n",
    "    \n",
    "def _make_target_columns(data_table):\n",
    "    matrix = data_table.matrix\n",
    "    matrix['LOG_REPAYMENT_ACTUAL'] = np.log1p(matrix['REPAYMENT_ACTUAL'])\n",
    "    matrix['PERCENT_ACTUAL'] = matrix['REPAYMENT_ACTUAL'] / matrix['CONTRACT_SUM'] * 100\n",
    "    matrix['LOG_PERCENT_ACTUAL'] = np.log1p(matrix['PERCENT_ACTUAL'])\n",
    "\n",
    "        \n",
    "def make_source_datatable(data_static, data_repay):\n",
    "    data_join = pd.merge(data_static, data_repay, on='CONTRACT_ID')\n",
    "    data_table = DataTable()\n",
    "    data_join['GENDER'] = 1 * (data_join['GENDER'] == 'M')\n",
    "    data_table.matrix = data_join\n",
    "    \n",
    "    _make_payment_cumulative_and_average_features(data_table)\n",
    "    _make_payment_history_features(data_table)\n",
    "    _make_payment_relative_features(data_table)\n",
    "    _make_payment_log_features(data_table)\n",
    "    \n",
    "    _make_regular_log_features(data_table)\n",
    "    _make_custom_features(data_table)\n",
    "    _make_is_grace_constant_feature(data_table)    \n",
    "    _make_target_columns(data_table)\n",
    "    return data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caring-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data_table):\n",
    "    contracts = np.array(data_table.matrix['CONTRACT_ID'].tolist())\n",
    "    contracts = np.unique(contracts)\n",
    "    np.random.shuffle(contracts)\n",
    "    length = contracts.shape[0]\n",
    "    RATIO = 0.1\n",
    "    test_contracts  = contracts[:int(length * RATIO)].tolist()\n",
    "    train_contracts = contracts[int(length * RATIO):].tolist()\n",
    "\n",
    "    data_learn = copy.deepcopy(data_table.matrix.dropna(axis=0))\n",
    "\n",
    "    data_train = copy.deepcopy(data_learn[data_learn['CONTRACT_ID'].isin(train_contracts)])\n",
    "    data_test  = copy.deepcopy(data_learn[data_learn['CONTRACT_ID'].isin(test_contracts)])\n",
    "    \n",
    "    train_table = copy.deepcopy(data_table)\n",
    "    train_table.matrix = data_train\n",
    "    test_table = copy.deepcopy(data_table)\n",
    "    test_table.matrix = data_test\n",
    "    dropna_table = copy.deepcopy(data_table)\n",
    "    dropna_table.matrix = data_learn\n",
    "    return train_table, test_table, dropna_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exclusive-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_table = make_source_datatable(data_static, data_repay)\n",
    "train_data_table, test_data_table, dropna_table = train_test_split(all_data_table)\n",
    "\n",
    "if not os.path.exists('prepared_data'):\n",
    "    os.makedirs('prepared_data')\n",
    "\n",
    "def check_existence(list_of_paths):\n",
    "    for item in list_of_paths:\n",
    "        if not os.path.exists(item):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "paths_list = ['prepared_data/train_data.csv', 'prepared_data/train_data.jbl', \n",
    "              'prepared_data/test_data.csv', 'prepared_data/test_data.jbl',\n",
    "              'prepared_data/all_data_dropped_na.csv', 'prepared_data/all_data_dropped_na.jbl',\n",
    "              'prepared_data/all_data_with_na.csv', 'prepared_data/all_data_with_na.jbl'\n",
    "             ]\n",
    "\n",
    "if not check_existence(paths_list):\n",
    "    train_data_table.matrix.to_csv('prepared_data/train_series.csv')\n",
    "    joblib.dump(train_data_table,  \"prepared_data/train_data.jbl\")\n",
    "    test_data_table.matrix.to_csv('prepared_data/test_series.csv')\n",
    "    joblib.dump(test_data_table,  \"prepared_data/test_data.jbl\")\n",
    "    all_data_table.matrix.to_csv('prepared_data/all_data_with_na.csv')\n",
    "    joblib.dump(all_data_table,  \"prepared_data/all_data_with_na.jbl\")\n",
    "    dropna_table.matrix.to_csv('prepared_data/all_data_dropped_na.csv')\n",
    "    joblib.dump(dropna_table,  'prepared_data/all_data_dropped_na.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-spending",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
