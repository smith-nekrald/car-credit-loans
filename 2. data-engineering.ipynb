{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs data engineering and exports the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing import Dict\n",
    "from typing import Any\n",
    "from typing import Optional\n",
    "from typing import Set\n",
    "from typing import Tuple\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_static: pd.DataFrame = pd.read_csv('raw_data/Static.csv', sep=';').drop('CLIENT_ID', axis=1)\n",
    "data_repay: pd.DataFrame  = pd.read_csv('raw_data/Repayments.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataTable API\n",
    "The cell below implements DataTable and API to feel it.\n",
    "The entry point is described in the function `make_source_datatable`, which takes `data_static` and `data_repay` as its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API for DataTable and feature engineering.\n",
    "\n",
    "class DataTable:\n",
    "    \"\"\" Stores DataFrame with engineered features, as well as \n",
    "    a structure over column names (which represent engineered features).\n",
    "    \n",
    "    Attributes:\n",
    "        matrix -- DataFrame with features.\n",
    "        \n",
    "        id_payment_features -- List with non-transformed payment-realated features names in original scale.\n",
    "        log_payment_features -- List with log-transformed payment-related features names in original scale.\n",
    "        id_relative_payment_features -- List with non-transformed payment-related features names in relative scale.\n",
    "        log_relative_payment_featurs -- List with log-transformed payment-related features names in relative scale.\n",
    "        \n",
    "        id_target_exact_key -- List with non-transformed target column key (next repayment) in original scale.\n",
    "        log_target_exact_key -- List with log-transformed target column key (next repayment) in original scale.\n",
    "        id_target_relative_key -- List with non-transformed target column key (next repayment) in relative scale.\n",
    "        log_target_relative_key -- List with log-transformed target column key (next repayment) in relative scale.\n",
    "        \n",
    "        explicit_target_keys -- List with all keys that represent target column.\n",
    "        indicator_features -- List with 0-1 indicator features names.\n",
    "        id_numeric_features -- List with non-transformed numeric features names.\n",
    "        log_numeric_features -- List with log-transformed numeric features names.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.matrix: Optional[pd.DataFrame] = None\n",
    "        \n",
    "        self.log_payment_features: List[str] = list()\n",
    "        self.id_payment_features: List[str]   = ['REPAYMENT_SCHEDULED']\n",
    "        self.id_relative_payment_features: List[str]  = list()\n",
    "        self.log_relative_payment_features: List[str] = list()\n",
    "        \n",
    "        self.id_target_exact_key: str  = 'REPAYMENT_ACTUAL'\n",
    "        self.log_target_exact_key: str = 'LOG_REPAYMENT_ACTUAL'\n",
    "        self.id_target_relative_key: str  = 'PERCENT_ACTUAL'\n",
    "        self.log_target_relative_key: str = 'LOG_PERCENT_ACTUAL'\n",
    "        \n",
    "        self.explicit_target_keys: List[str] = [ \n",
    "            self.id_target_exact_key, self.log_target_exact_key, \n",
    "            self.id_target_relative_key, self.log_target_relative_key\n",
    "        ]\n",
    "\n",
    "        self.indicator_features: List[str] = ['GENDER']\n",
    "        self.id_numeric_features: List[str]  = [\n",
    "            'PERIOD_ID', 'TERM', 'CONTRACT_SUM', 'AGE', \n",
    "            'LOAN_TO_INCOME', 'PAYMENT_TO_INCOME', 'DOWNPAYMENT', \n",
    "            'CAR_CATEGORY', 'GRACE_PERIOD', 'RATE_CHANGE_AFTER_GRACE'\n",
    "        ]\n",
    "        self.log_numeric_features: List[str] = list()\n",
    "        \n",
    "\n",
    "def _make_payment_cumulative_and_average_features(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds non-transformed cumulative and average \n",
    "    payment features to the DataTable. \n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "    data: pd.DataFrame = data_table.matrix\n",
    "    group_structure: pd.GroupBy = data.groupby('CONTRACT_ID')\n",
    "    \n",
    "    cumulative_sum_scheduled: List[float] = list()\n",
    "    cumulative_sum_actual: List[float] = list()\n",
    "    average_repayment_actual: List[float] = list()\n",
    "    \n",
    "    key: str; df: pd.DataFrame\n",
    "    for key, df in group_structure:\n",
    "        cumulative_sum_scheduled += df['REPAYMENT_SCHEDULED'].cumsum().tolist()\n",
    "        cumulative_sum_actual += (df['REPAYMENT_ACTUAL'].cumsum() - df['REPAYMENT_ACTUAL']).tolist()\n",
    "        \n",
    "        cumsum_actual: Union[np.array, List[float]] = np.array(df['REPAYMENT_ACTUAL'].cumsum().tolist())\n",
    "        counts_actual: Union[np.array, List[int]] = np.array(range(1, len(cumsum_actual) + 1))\n",
    "        average_actual: Union[np.array, List[float]] = cumsum_actual / counts_actual\n",
    "        corrected_average_history: List[float] = [0.]\n",
    "        \n",
    "        item: float\n",
    "        for item in average_actual.tolist():\n",
    "            corrected_average_history.append(item)\n",
    "        corrected_average_history.pop()\n",
    "        average_repayment_actual += corrected_average_history\n",
    "        \n",
    "    data['CUMSUM_REPAYMENT_SCHEDULED'] = cumulative_sum_scheduled\n",
    "    data['CUMSUM_REPAYMENT_ACTUAL'] = cumulative_sum_actual\n",
    "    data['AVERAGE_REPAYMENT_ACTUAL'] = average_repayment_actual\n",
    "    data_table.id_payment_features += [\n",
    "        'CUMSUM_REPAYMENT_SCHEDULED', 'CUMSUM_REPAYMENT_ACTUAL', 'AVERAGE_REPAYMENT_ACTUAL']\n",
    "        \n",
    "\n",
    "def _make_payment_history_features(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds payment history features to the DataTable. \n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    data: pd.DataFrame = data_table.matrix\n",
    "    group_structure: pd.GroupBy = data.groupby('CONTRACT_ID')\n",
    "    \n",
    "    lagged_actual_payments: Dict[int, List[float]] = defaultdict(list)\n",
    "    lagged_scheduled_payments: Dict[int, List[float]] = defaultdict(list)\n",
    "    \n",
    "    configurations: List[Tuple[List[float], int, str]] = [\n",
    "        (lagged_actual_payments[1], 1, 'REPAYMENT_ACTUAL'),\n",
    "        (lagged_scheduled_payments[1], 1, 'REPAYMENT_SCHEDULED'),\n",
    "        (lagged_actual_payments[2], 2, 'REPAYMENT_ACTUAL'),\n",
    "        (lagged_scheduled_payments[2], 2, 'REPAYMENT_SCHEDULED')\n",
    "    ]\n",
    "                         \n",
    "    key: str; df: pd.DataFrame\n",
    "    list_holder: List[float]; shift: int; key: str\n",
    "    for key, df in group_structure:\n",
    "        for list_holder, shift, key in configurations:\n",
    "            contract_list: List[float] = df[key].shift(shift).tolist()\n",
    "            idx: int\n",
    "            for idx in range(shift):\n",
    "                contract_list[idx] = df['REPAYMENT_SCHEDULED'].values[idx]\n",
    "            list_holder += contract_list\n",
    "    \n",
    "    for list_holder, shift, key in configurations:\n",
    "        new_key: str = 'HISTORY_SHIFT_{}_'.format(shift) + key\n",
    "        data[new_key] = list_holder\n",
    "        data_table.id_payment_features.append(new_key)\n",
    "        \n",
    "\n",
    "def _make_payment_relative_features(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds relative payment features to the DataTable. \n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "    matrix: pd.DataFrame = data_table.matrix\n",
    "    exact_features: List[str] = copy.deepcopy(data_table.id_payment_features)\n",
    "    \n",
    "    item: str\n",
    "    for item in exact_features:\n",
    "        relative_key: str = 'RELATIVE_' + item\n",
    "        matrix['RELATIVE_' + item] = matrix[item] / matrix['CONTRACT_SUM'] * 100\n",
    "        data_table.id_relative_payment_features.append(relative_key)\n",
    "\n",
    "        \n",
    "def _make_payment_log_features(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds log-transformed payment features to the DataTable. \n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "    matrix: pd.DataFrame = data_table.matrix\n",
    "    configurations: List[Tuple[List[str], List[str]]] = [\n",
    "        (data_table.id_payment_features, data_table.log_payment_features),\n",
    "        (data_table.id_relative_payment_features, data_table.log_relative_payment_features)\n",
    "    ]\n",
    "    source: List[str]; target: List[str]\n",
    "    for source, target in configurations:\n",
    "        feature_key: str\n",
    "        for feature_key in source:\n",
    "            log_key: str = 'LOG_' + feature_key\n",
    "            matrix[log_key] = np.log1p(matrix[feature_key])\n",
    "            target.append(log_key)\n",
    "    \n",
    "\n",
    "def _make_regular_log_features(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds regular log-transformed features to the DataTable. \n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "    matrix: pd.DataFrame = data_table.matrix\n",
    "    feature_key: str\n",
    "    for feature_key in data_table.id_numeric_features:\n",
    "        log_key: str = 'LOG_' + feature_key\n",
    "        matrix[log_key] = np.log1p(matrix[feature_key])\n",
    "        data_table.log_numeric_features.append(log_key)\n",
    "        \n",
    "        \n",
    "def _make_custom_features(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds custom features to the DataTable. \n",
    "    This includes indicators if the grace is on, the ratio thresholds\n",
    "    of loan size to income, number of period and type of car. \n",
    "    Also, \"before grace\" feature is added, which describes payment\n",
    "    to load ratio. This way it allows to distinguish grace-free and\n",
    "    regular periods.\n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "    matrix: pd.DataFrame = data_table.matrix\n",
    "    matrix['BEFORE_GRACE'] = matrix['PAYMENT_TO_INCOME'] / matrix['LOAN_TO_INCOME'] * 100\n",
    "    matrix['GRACE_ON'] = 1 * np.array(matrix['PERIOD_ID'] <= matrix['GRACE_PERIOD'])\n",
    "    \n",
    "    matrix['RATIO_5'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 5)\n",
    "    matrix['RATIO_10'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 10)\n",
    "    matrix['RATIO_20'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 20)\n",
    "    matrix['RATIO_30'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 30)\n",
    "    matrix['RATIO_40'] = 1 * np.array(matrix['LOAN_TO_INCOME'] >= 40)\n",
    "    \n",
    "    matrix['IS_PERIOD_1'] = 1 * np.array(matrix['PERIOD_ID'] == 1)\n",
    "    matrix['IS_PERIOD_2'] = 1 * np.array(matrix['PERIOD_ID'] == 2)\n",
    "    matrix['IS_PERIOD_3'] = 1 * np.array(matrix['PERIOD_ID'] == 3)\n",
    "    \n",
    "    matrix['IS_CAR_1'] = 1 * np.array(matrix['CAR_CATEGORY'] == 1)\n",
    "    matrix['IS_CAR_2'] = 1 * np.array(matrix['CAR_CATEGORY'] == 2)\n",
    "    matrix['IS_CAR_3'] = 1 * np.array(matrix['CAR_CATEGORY'] == 3)\n",
    "    matrix['IS_CAR_4'] = 1 * np.array(matrix['CAR_CATEGORY'] == 4)\n",
    "    matrix['IS_CAR_5'] = 1 * np.array(matrix['CAR_CATEGORY'] == 5)\n",
    "    \n",
    "    data_table.id_numeric_features.append('BEFORE_GRACE')\n",
    "    data_table.indicator_features += ['GRACE_ON', 'RATIO_5', 'RATIO_10', 'RATIO_20', 'RATIO_30', 'RATIO_40'\n",
    "                                     'IS_PERIOD_1', 'IS_PERIOD_2', 'IS_PERIOD_3',\n",
    "                                     'IS_CAR_1', 'IS_CAR_2', 'IS_CAR_3', 'IS_CAR_4', 'IS_CAR_5']\n",
    "\n",
    "\n",
    "def _make_is_grace_constant_feature(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds is_grace_constant feature to the DataTable.\n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "    is_grace_constant: List[int] = list()\n",
    "    matrix: pd.DataFrame = data_table.matrix\n",
    "        \n",
    "    key: str; df: pd.DataFrame\n",
    "    for key, df in matrix.groupby('CONTRACT_ID'):\n",
    "        df_grace_on: bool = np.array(df['GRACE_ON'])\n",
    "        check_vals: Union[np.array, List[float]]  = np.array(df['REPAYMENT_ACTUAL'])\n",
    "        grace_values: Set[float] = set()\n",
    "        on: int; val: float\n",
    "        for on, val in zip(df_grace_on, check_vals):\n",
    "            if on == 1:\n",
    "                grace_values.add(val)\n",
    "        append: Union[np.array, List[float]] = np.zeros_like(check_vals)\n",
    "        if len(grace_values) < 2:\n",
    "            append += 1\n",
    "        is_grace_constant += append.tolist()\n",
    "\n",
    "    matrix['IS_GRACE_CONSTANT'] = np.array(is_grace_constant)\n",
    "    data_table.indicator_features.append('IS_GRACE_CONSTANT')\n",
    "\n",
    "    \n",
    "def _make_target_columns(data_table: DataTable) -> None:\n",
    "    \"\"\" Adds differently transformed target columns to the DataTable.\n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \"\"\"\n",
    "\n",
    "    matrix: pd.DataFrame = data_table.matrix\n",
    "    matrix['LOG_REPAYMENT_ACTUAL'] = np.log1p(matrix['REPAYMENT_ACTUAL'])\n",
    "    matrix['PERCENT_ACTUAL'] = matrix['REPAYMENT_ACTUAL'] / matrix['CONTRACT_SUM'] * 100\n",
    "    matrix['LOG_PERCENT_ACTUAL'] = np.log1p(matrix['PERCENT_ACTUAL'])\n",
    "\n",
    "        \n",
    "def make_source_datatable(data_static: pd.DataFrame, data_repay: pd.DataFrame) -> DataTable:\n",
    "    \"\"\" Performs Feature Engineering and builds DataTable with corresponding information.\n",
    "    \n",
    "    Arguments:\n",
    "        data_static: DataFrame with static information.\n",
    "        data_repay: DataFrame with information about repayments.\n",
    "    \n",
    "    Returns:\n",
    "        DataTable -- ready to use structure with engineered features.\n",
    "    \"\"\"\n",
    "    data_join: pd.DataFrame = pd.merge(data_static, data_repay, on='CONTRACT_ID')\n",
    "    data_table: DataTable = DataTable()\n",
    "    data_join['GENDER'] = 1 * (data_join['GENDER'] == 'M')\n",
    "    data_table.matrix = data_join\n",
    "    \n",
    "    _make_payment_cumulative_and_average_features(data_table)\n",
    "    _make_payment_history_features(data_table)\n",
    "    _make_payment_relative_features(data_table)\n",
    "    _make_payment_log_features(data_table)\n",
    "    \n",
    "    _make_regular_log_features(data_table)\n",
    "    _make_custom_features(data_table)\n",
    "    _make_is_grace_constant_feature(data_table)    \n",
    "    _make_target_columns(data_table)\n",
    "    \n",
    "    return data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data_table: DataTable) -> Tuple[DataTable, DataTable, DataTable]:\n",
    "    \"\"\" Performs train-test split.\n",
    "    \n",
    "    Arguments:\n",
    "        data_table: Structure storing DataFrame with engineered features\n",
    "            alongside with convenient column names.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple with three DataTables. The first is training dataset, the second is\n",
    "        testing dataset, and the third is complete dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    contracts: List[str] = np.array(data_table.matrix['CONTRACT_ID'].tolist())\n",
    "    contracts: Tuple[List[str], List[int]] = np.unique(contracts)\n",
    "    np.random.shuffle(contracts)\n",
    "    length: int = contracts.shape[0]\n",
    "    RATIO: float = 0.1\n",
    "    test_contracts: List[str]  = contracts[:int(length * RATIO)].tolist()\n",
    "    train_contracts: List[str] = contracts[int(length * RATIO):].tolist()\n",
    "\n",
    "    data_learn: pd.DataFrame = copy.deepcopy(data_table.matrix.dropna(axis=0))\n",
    "\n",
    "    data_train: pd.DataFrame = copy.deepcopy(data_learn[data_learn['CONTRACT_ID'].isin(train_contracts)])\n",
    "    data_test: pd.DataFrame  = copy.deepcopy(data_learn[data_learn['CONTRACT_ID'].isin(test_contracts)])\n",
    "    \n",
    "    train_table: DataTable = copy.deepcopy(data_table)\n",
    "    train_table.matrix = data_train\n",
    "    test_table: DataTable = copy.deepcopy(data_table)\n",
    "    test_table.matrix = data_test\n",
    "    dropna_table: DataTable = copy.deepcopy(data_table)\n",
    "    dropna_table.matrix = data_learn\n",
    "    \n",
    "    return train_table, test_table, dropna_table\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logic calling defined API above and saving the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_table: DataTable = make_source_datatable(data_static, data_repay)\n",
    "train_data_table: DataTable; test_data_table: DataTable; dropna_table: DataTable\n",
    "train_data_table, test_data_table, dropna_table = train_test_split(all_data_table)\n",
    "\n",
    "if not os.path.exists('prepared_data'):\n",
    "    os.makedirs('prepared_data')\n",
    "\n",
    "def check_existence(list_of_paths: List[str]) -> bool:\n",
    "    \"\"\" Checks if all of the paths exist in operating system. \n",
    "    \n",
    "    Arguments:\n",
    "        list_of_paths -- List containing paths to check.\n",
    "        \n",
    "    Returns:\n",
    "        True if all of the paths are valid, and False otherwise.\n",
    "    \"\"\"\n",
    "    item: bool\n",
    "    for item in list_of_paths:\n",
    "        if not os.path.exists(item):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "paths_list: List[str] = ['prepared_data/train_data.csv', 'prepared_data/train_data.jbl', \n",
    "              'prepared_data/test_data.csv', 'prepared_data/test_data.jbl',\n",
    "              'prepared_data/all_data_dropped_na.csv', 'prepared_data/all_data_dropped_na.jbl',\n",
    "              'prepared_data/all_data_with_na.csv', 'prepared_data/all_data_with_na.jbl'\n",
    "             ]\n",
    "\n",
    "if not check_existence(paths_list):\n",
    "    train_data_table.matrix.to_csv('prepared_data/train_series.csv')\n",
    "    joblib.dump(train_data_table,  \"prepared_data/train_data.jbl\")\n",
    "    test_data_table.matrix.to_csv('prepared_data/test_series.csv')\n",
    "    joblib.dump(test_data_table,  \"prepared_data/test_data.jbl\")\n",
    "    all_data_table.matrix.to_csv('prepared_data/all_data_with_na.csv')\n",
    "    joblib.dump(all_data_table,  \"prepared_data/all_data_with_na.jbl\")\n",
    "    dropna_table.matrix.to_csv('prepared_data/all_data_dropped_na.csv')\n",
    "    joblib.dump(dropna_table,  'prepared_data/all_data_dropped_na.jbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
